{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Siri-cod/LLMcourse/blob/main/homework/04_evaluation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BE3y75m_lSE0"
      },
      "source": [
        "Homework 4: LLM evaluation (59 points)\n",
        "=====\n",
        "\n",
        "The third homework zooms in on evaluating LLMs, specifically, on the following skills: using log probabilities of string under a trained LM to evaluate it, coming up with items to test particular aspets of LLMs, and comparing LLM measures to measures of human performance.\n",
        "\n",
        "### Logistics\n",
        "\n",
        "* submission deadline: July 15th th 23:59 German time via Moodle\n",
        "  * please upload a **SINGLE .IPYNB FILE named Surname_FirstName_HW4.ipynb** containing your solutions of the homework. Make sure that your **plots** for the last exercise are either rendered in the notebook or submitted together with it in a zip file.\n",
        "* please solve and submit the homework **individually**!\n",
        "* if you use Colab, to speed up the execution of the code on Colab, you can use the available GPU (if Colab resources allow). For that, before executing your code, navigate to Runtime > Change runtime type > GPU > Save.\n",
        "\n",
        "**NOTE:** It might be that, on Colab, you run out of memory between the exercises. You might need to restart the runtime / it will restart automatically. After the restart, you might need to re-install packages. Furthermore, if you work on Colab, do not leave working on these tasks to the last minute, as Colab sometimes limits access to GPU runtime / day if one tries to restart access GPUs frequently."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZibVnAilSE2"
      },
      "source": [
        "## Exercise 1: Understanding grammatical capabilities of LLMs (10 points)\n",
        "\n",
        "In this task, we look at [BLiMP](https://aclanthology.org/2020.tacl-1.25/), the benchmark of linguistic minimal pairs. This is a well-known benchmark for evaluating linguistic capabilities of language models. It consists of 67 individual datasets, each containing 1,000 minimal pairs -- that is, pairs of minimally different sentences that contrast in grammatical acceptability and isolate specific phenomenon in syntax, morphology, or semantics. The authors suggest to use the benchmark to evaluate LMs by observing whether they assign a higher probability to the acceptable sentence in each minimal pair.\n",
        "\n",
        "> Your task is to evaluate an open-source model, [Olmo-2-1B-Instruct](https://huggingface.co/allenai/OLMo-2-0425-1B-Instruct), on this benchmark by completing the code below. Based on your evaluation results, please answer the following questions.\n",
        "> Please use the following test suites to answer them: anaphor_gender_agreement, determiner_noun_agreement_with_adjective_1, animate_subject_passive, complex_NP_island, npi_present_1, superlative_quantifiers_1, existential_there_object_raising, principle_A_case_1.\n",
        ">\n",
        "> The entire benchmark can be found [here](https://huggingface.co/datasets/nyu-mll/blimp).\n",
        ">\n",
        "> 1. Plot the accuracy of the model on the different grammatical phenomena, represented in different test suites.\n",
        "> 2. Calculate the average accuracies and the confidence intervals in the different fields: syntax, morphology, syntax-semantics, semantics. Is the performance the same across the different fields? Which field is the most difficult one?\n",
        "> 3. What is the easiest grammatical phenomenon, what is the most difficult grammatical phenomenon (as captured by the single test suites) for the model?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9enrajIwlSE2"
      },
      "source": [
        "Note: you might be prompted to reload your runtime after installing minicons, and you might see some deprecation warnings / dependency incompatibilities when it is installed. Please ignore these warnings (usually not the best practice, we know!)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GZCtUXHClSE2"
      },
      "outputs": [],
      "source": [
        "#!pip install minicons datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "48XJ-flylSE3"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "import torch\n",
        "from minicons import scorer\n",
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u2H2ESDhlSE3"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device('cuda')\n",
        "elif torch.backends.mps.is_available():\n",
        "    device = torch.device('mps')\n",
        "else:\n",
        "    device = torch.device('cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qtx-RqtglSE3"
      },
      "outputs": [],
      "source": [
        "# iterate over the test suites\n",
        "\n",
        "#### YOUR CODE HERE ####\n",
        "dataset = load_dataset(\"nyu-mll/blimp\", #### YOUR TEST SUITE HERE ####)\n",
        "# inspect the dataset\n",
        "dataset[\"train\"][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "khiFn10mlSE3"
      },
      "outputs": [],
      "source": [
        "# iterate over the single items of the test suite\n",
        "# hint: you can use code similar to the one in sheet 7.1\n",
        "\n",
        "# set up the model as a minicons scorer\n",
        "lm_scorer = scorer.IncrementalLMScorer(\n",
        "    ### YOUR CODE HERE ###\n",
        ")\n",
        "\n",
        "# create some lists to store the results\n",
        "### YOUR CODE HERE ###\n",
        "\n",
        "for item in dataset[\"train\"]:\n",
        "    # get the sentence pair\n",
        "    ### YOUR CODE HERE ###\n",
        "\n",
        "    # compare the sentences as suggested in the task description\n",
        "    ### YOUR CODE HERE ###"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cnEXRRZ1lSE3"
      },
      "outputs": [],
      "source": [
        "# calculate the performance by test suite\n",
        "### YOUR CODE HERE ###\n",
        "# plot the results in a bar plot\n",
        "### YOUR CODE HERE ###"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mKZlcy1rlSE4"
      },
      "outputs": [],
      "source": [
        "# calculate the performance as described above by category and plot the results in a bar plot with CIs\n",
        "### YOUR CODE HERE ###"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pth1AXdQlSE4"
      },
      "source": [
        "## Exercise 2: Evaluating societal biases (13 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Crvjc9PLlSE4"
      },
      "source": [
        "In this exercise, we will consider aspects of LLM performance which may have social implications and that are deeply interconnected with how humans use language. This task evaluates whether LLMs overrepresent certain cultures over others, which could be due to, e.g., imbalances over training data sources and languages.\n",
        "\n",
        "> Specifically, your task is to come up with an appropriate test item and evaluate whether LLMs exhibit certain cultural biases.\n",
        "> In this task, you have to construct your own multiple-choice test item for investigating cultural biases of LLMs, where, given a context, the different available response / continuation options would reflect preferences for responses typical for different cultures.\n",
        "> For instance, one response could be more acceptable under one particular cultural lens and another response under a different cultural background.\n",
        "> Your task is then to evaluate the performance of two LLMs: the mostly monolingual `gpt2-large` and the multilingual `bigscience/bloom-560m` model. The second part of the task is to complete the evaluation code and interpret the results by answering the question below.\n",
        "> Note that there obviously is no signle right answer to this, so we will assess the plausibility of your idea and the correctness of the code & interpretation.\n",
        "\n",
        "Here is a simple example of a test item. More explanations are in parentheses. You should provide analogous explanations in the answers to the questions below, but do not pass these to the LLMs during evaluations.\n",
        "\n",
        "\n",
        "Context 1: You are at a German supermarket. You walk up to the cashier and greet them by saying:\n",
        "\n",
        "Context 2: You are at an American supermarket. You walk up to the cashier and greet them by saying:\n",
        "\n",
        "A. Hello. (intuititvely, more likely in to be appropiate in the Germany context condition)\n",
        "\n",
        "B. Bye. (a generally inappropriate response)\n",
        "\n",
        "C. Hello, how are you? (intuitively, more likely to be appropriate in the US context condition; people usually don’t ask strangers ‘how are you’ in Germany)\n",
        "\n",
        "I would say: (insert each of the answer options separately here and calculate their log probability, given each of the contexts).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xz5xnmtElSE4"
      },
      "source": [
        "For reference about constructing datasets and inspiration, feel free to take a look at the [ETHICS dataset](https://arxiv.org/pdf/2008.02275), e.g., Fig. 2, where the authors came up with different continuations tapping into different conditions, given a context.\n",
        "\n",
        "> **Fill in your responses below.**\n",
        ">\n",
        "> 1. Your prompt (context 1 and 2, response options A-C; with explanations of the intuitive differences for each response option in respective cultural contexts):\n",
        "> 2. Your model log probabilities (please fill in the table below with the conditional log probabilities you retrieved with the code below for your respective item):\n",
        ">\n",
        "| Context / Option | GPT-2 | Bloom |\n",
        "|------------------|-------|-------|\n",
        "| context 1 + A      | ...   | ...   |\n",
        "| context 2 + A          |       |       |\n",
        "| context 1 + B      |       |       |\n",
        "| context 2 + B          |       |       |\n",
        "| context 1 + C         |       |       |\n",
        "| context 2 + C         |       |       |\n",
        "\n",
        "> 3. Do the models show a preference for a particular cultural setting (context 1 or context 2)? Is there evidence for whether cultural biases might be caused by training data?\n",
        "> 4. Are there aspects of the prompt that might influence your results? Please provide a brief justification / example why (not). Write 2 sentences max."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B07kz1pClSE4"
      },
      "outputs": [],
      "source": [
        "from minicons import scorer\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qBAzeQoNlSE4"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device('cuda')\n",
        "elif torch.backends.mps.is_available():\n",
        "    device = torch.device(\"mps\")\n",
        "else:\n",
        "    device = torch.device('cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aaQ7xyJZlSE4"
      },
      "outputs": [],
      "source": [
        "# here is some starter code; please fill in your code / comments where it says #### YOUR CODE / COMMENT HERE ####\n",
        "\n",
        "# set up a scorer\n",
        "gpt2_scorer = scorer.IncrementalLMScorer(\n",
        "    ### YOUR CODE HERE ###\n",
        ")\n",
        "\n",
        "bloom_scorer = scorer.IncrementalLMScorer(\n",
        "    ### YOUR CODE HERE ###\n",
        ")\n",
        "# initialize list for storing the predictions\n",
        "gpt2_predictions = []\n",
        "bloom_predictions = []\n",
        "answer_keys = [\"ger\", \"nonsense\", \"us\"]\n",
        "\n",
        "# iterate over contexts\n",
        "for context in range(### YOUR CODE HERE ###):\n",
        "    # format / provide  the possible answer options from your vignette\n",
        "    answer_options = ### YOUR CODE HERE ###\n",
        "    # pass a list of contexts and a list of continuations to be scored\n",
        "    answer_scores_gpt2 = gpt2_scorer.conditional_score(\n",
        "        # format the question into a list of same length as the number of answer options\n",
        "        ### YOUR CODE HERE ###,\n",
        "    )\n",
        "    answer_scores_bloom = bloom_scorer.conditional_score(\n",
        "        # format the question into a list of same length as the number of answer options\n",
        "        ### YOUR CODE HERE ###,\n",
        "    )\n",
        "\n",
        "    # check / inspect which answer has the highest score and which answer type (i.e., \"culture\") it corresponds to\n",
        "    ### YOUR CODE / COMMENT HERE ###"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KYElvwD-lSE5"
      },
      "source": [
        "## Exercise 3: Theory of mind evaluations of LLMs (14 points)\n",
        "\n",
        "One capability that has been much debated in the context oflanguage models is so-called *theory-of-mind* (ToM) reasoning, a capability to reason about other people's latent mental states like beliefs and knowledge and how they influence the person's behavior.\n",
        "\n",
        "One well-known task to investigate theory of mind capabilities is the so-called False Belief task.\n",
        "We will use a benchmark [BigToM](https://arxiv.org/abs/2306.15448) that contains items with variations of this task generated trough LMs, specifically a subset of the forward and backward belief conditions.\n",
        "Please download the relevant data [here](https://github.com/CogSciPrag/Understanding-LLMs-course/tree/main/understanding-llms/homework/data/bigtom.csv).\n",
        "\n",
        "> Your task here is to:\n",
        ">\n",
        "> 1. Take a look at the paper to familiarize yourself with the benchmark. In max. two sentences per condition, explain what the forward and backward belief conditions of the benchmark test.\n",
        "> 2. Complete the following code to evaluate the Olmo model from exercise 1 on the ToM benchmark (fill in the slots with ###YOUR CODE / COMMENT HERE ###). Specifically:\n",
        ">\n",
        ">   a. Evaluate the performance of the model when scoring the linguistic continuations of the sentence and comparing the respective conditional log probabilities. Compare the performance of the model in the false belief and the true belief conditions.\n",
        ">\n",
        ">   b. Evaluate the performance of the model when formatting the prompt in a multiple choice format, where the two continuations for each prompt are included in the prompt along with labels (e.g., A., B.). Then, let the model generate a few tokens and check if the correct answer label was predicted. . Compare the performance of the model in the false belief and the true belief conditions.\n",
        ">\n",
        ">   c. Does the model perform better under one of the evaluating approaches? Is it similar across true and false beliefs?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-wJMQSANlSE5"
      },
      "outputs": [],
      "source": [
        "# note: only load the model again if you are in a different runtime than when you solved exercise 1\n",
        "# lm_scorer = scorer.IncrementalLMScorer(\n",
        "    # \"allenai/OLMo-2-0425-1B-Instruct\",\n",
        "    # device\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3nLKNadhlSE5"
      },
      "outputs": [],
      "source": [
        "# read the data (Note: you have to upload the file to your session in colab)\n",
        "tom_back = pd.read_csv(\"data/bigtom.csv\")\n",
        "\n",
        "# inspect the data and construct the evaluation prompts for standard scoring of sentence completions\n",
        "tom_back[\"prompt_tb\"] = #### YOUR CODE HERE ####\n",
        "tom_back[\"prompt_fb\"] = #### YOUR CODE HERE ####\n",
        "\n",
        "# calculate the scores of the true belief condition with the true continuations\n",
        "tb_scores_true = lm_scorer.conditional_score(\n",
        "    # format the prompt into a list of same length as the number of answer options\n",
        "    tom_back[\"prompt_tb\"].tolist(),\n",
        "    #### YOUR CODE HERE ####\n",
        ")\n",
        "# calculate the scores of the true belief condition with the false continuations\n",
        "tb_scores_false = lm_scorer.conditional_score(\n",
        "    tom_back[\"prompt_tb\"].tolist(),\n",
        "    #### YOUR CODE HERE ####\n",
        ")\n",
        "\n",
        "# calculate the accuracy of the true belief condition\n",
        "#### YOUR CODE HERE ####\n",
        "print(\"True belief accuracy main \", tb_accuracy.mean())\n",
        "\n",
        "# calculate the scores of the false belief condition with the true continuations\n",
        "fb_scores_true = lm_scorer.conditional_score(\n",
        "    ### YOUR CODE HERE ###,\n",
        "\n",
        ")\n",
        "# calculate the scores of the false belief condition with the false continuations\n",
        "fb_scores_false = lm_scorer.conditional_score(\n",
        "    #### YOUR CODE HERE ####\n",
        ")\n",
        "# calculate the accuracy of the false belief condition\n",
        "#### YOUR CODE HERE ####\n",
        "print(\"False belief accuracy main \", fb_accuracy.mean())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yE9oWe1wlSE5"
      },
      "outputs": [],
      "source": [
        "# create the prompt for the multiple choice condition\n",
        "def create_forced_choice_prompt(row):\n",
        "    \"\"\"\n",
        "    Construct a prompt with the story, the trigger sentence,\n",
        "    the task to choose the correct continuation for the the final sentence prefix,\n",
        "    and the two continuations appended to the labels A, B.\n",
        "    The order of the continuations is shuffled.\n",
        "\n",
        "    Input:\n",
        "    ------\n",
        "    row: pandas Series\n",
        "        Contains the story, trigger sentences, final sentence prefix, and continuations\n",
        "\n",
        "    Returns:\n",
        "    ------\n",
        "    prompt_forced_choice_tb: str\n",
        "        Forced choice prompt for true belief condition\n",
        "    prompt_forced_choice_fb: str\n",
        "        Forced choice prompt for false belief condition\n",
        "    correct_label_tb: str\n",
        "        Correct label for the correct true belief continuation (either \"A\" or \"B\", depending on the shuffling of the order of continuations)\n",
        "    correct_label_fb: str\n",
        "        Correct label for the correct false belief continuation (either \"A\" or \"B\", depending on the shuffling of the order of continuations)\n",
        "    \"\"\"\n",
        "    # shuffle the order of the true and false continuations for both conditions\n",
        "\n",
        "    tb_order = np.random.choice([\"tb_correct_continuation\", \"fb_correct_continuation\"], replace=False, size=2).tolist()\n",
        "    fb_order = np.random.choice([\"tb_correct_continuation\", \"fb_correct_continuation\"], replace=False, size=2).tolist()\n",
        "    task = \" Choose the correct continuation for the following sentence: \"\n",
        "    # create the prompt for the forced choice task\n",
        "    prompt_forced_choice_tb = ### YOUR CODE HERE ###\n",
        "    prompt_forced_choice_fb = ### YOUR CODE HERE ###\n",
        "    # get the correct labels for the true and false belief conditions\n",
        "    correct_label_tb = [\"A\", \"B\"][### YOUR CODE HERE ###]\n",
        "    correct_label_fb = [\"A\", \"B\"][### YOUR CODE HERE ###]\n",
        "    return prompt_forced_choice_tb, prompt_forced_choice_fb, correct_label_tb, correct_label_fb\n",
        "\n",
        "\n",
        "tom_fc_generation = tom_back.copy()\n",
        "\n",
        "tom_fc_generation[[\n",
        "    \"prompt_forced_choice_tb\",\n",
        "    \"prompt_forced_choice_fb\",\n",
        "    \"correct_label_tb\",\n",
        "    \"correct_label_fb\"\n",
        "]] = tom_back.apply(\n",
        "    lambda row: create_forced_choice_prompt(row),\n",
        "    axis=1,\n",
        "    result_type=\"expand\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fn7KQuU8lSE5"
      },
      "outputs": [],
      "source": [
        "# Now evaluate the forced choice condition with generation\n",
        "free_generation_accuracy_tb = []\n",
        "free_generation_accuracy_fb = []\n",
        "\n",
        "# set up the generation pipeline\n",
        "\n",
        "from transformers import pipeline\n",
        "generation_pipeline = pipeline(\n",
        "    \"text-generation\",\n",
        "    model = lm_scorer.model,\n",
        "    device=device,\n",
        "    tokenizer=lm_scorer.tokenizer,\n",
        ")\n",
        "\n",
        "for i, r in tom_fc_generation.iterrows():\n",
        "    # generate the continuation for the true belief condition\n",
        "    pred = generation_pipeline(\n",
        "        ### YOUR CODE HERE ###\n",
        "        max_length=5,\n",
        "        temperature=0.7\n",
        "    )\n",
        "    # check if the prediction contains the correct label\n",
        "    ### YOUR CODE HERE ###\n",
        "        free_generation_accuracy_tb.append(1)\n",
        "    else:\n",
        "        free_generation_accuracy_tb.append(0)\n",
        "    # generate the continuation for the false belief condition\n",
        "    pred = generation_pipeline(\n",
        "        ### YOUR CODE HERE ###\n",
        "        max_length=5,\n",
        "        temperature=0.7\n",
        "    )\n",
        "    # check if the prediction contains the correct label\n",
        "    ### YOUR CODE HERE ###\n",
        "        free_generation_accuracy_fb.append(1)\n",
        "    else:\n",
        "        free_generation_accuracy_fb.append(0)\n",
        "\n",
        "# print the results\n",
        "### YOUR CODE HERE ###"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1VOV4-8ulSE5"
      },
      "source": [
        "## Exercise 4: How human-like are Llama's surprisals? (22 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kyM6Un0VlSE5"
      },
      "source": [
        "More recently, work more informed by human language use and processing has compared LLMs’ performance to aspects of human behavior. Here, the assessment of LLMs is guided more by the question of how human-like certain aspects of its performance are. For instance, we might whether LLMs' 'knowledge' of language is comparable to human knowledge, and, in particular, whether the processing of language, given the knowledge , can be compared via system-appropriate linking measurements.\n",
        "\n",
        "Your task in this exercise is to assess whether the *surprisal* of different language models is comparable to human *reading times*, when it comes to processing subject-verb agreement. The linking hypothesis is that these can be considered the respective predictability, and therefore, processing load indicators.\n",
        "The conceptual ideas and the data are taken from [Wilcox et al. (2021)](https://arxiv.org/pdf/2106.03232) which was discussed in the lecture. Please read the sections 1-2.2 for the background (optionally, the rest, if you want).\n",
        "The data can be downloaded [here](https://github.com/CogSciPrag/Understanding-LLMs-course/tree/main/understanding-llms/homework/data/SVA_data.csv).\n",
        "\n",
        "The data provides human RTs and LM surprisals in different conditions for sentences where the subject and the verb either match (i.e., agree) or mismatch in terms of number. This is the main condition. Furthermore, the agreement manipulation occurs in different syntactic conditions, and for plural and singular nouns. Here are examples from the different syntactic conditions:\n",
        "* SRC (subject relative clause modifier):\n",
        "  * mismatch plural: The pilots that injured the teacher brings love to people.\n",
        "  * match plural: The pilots that injured the teacher bring love to people.\n",
        "* ORC (object relative clause modifier):\n",
        "  * mismatch plural: The ministers that the manager injured knows tennis.\n",
        "  * match plural: The ministers that the manager injured know tennis.\n",
        "* PP (prepositional phrase modifier):\n",
        "  * mismatch plural: The executives next to the teacher is good.\n",
        "  * match plural: The executives next to the teacher are good.\n",
        "\n",
        "The prediction is that humans and models should have difficulty processing the mismatched noun, both in the singular and the plural condition.\n",
        "\n",
        "> Your task is to complete / provide the following code and answer the following questions:\n",
        "> 1. Formulate a quantitatively testable hypothesis operationalizing the prediction above. I.e., formulate something like: if the prediction is true, X should be larger than Y.\n",
        "> 2. Provide respective examples for the singular condition.\n",
        "> 3. Inspect the data. What are the units of the provided results?\n",
        "> 4. Based on your hypothesis above, for each trial, calculate whether it holds or not. Plot the proportion of trials where your hypothesis is borne out (i.e, the accuracy), for humans and each model, in the singular and the plural condition. (Hint: use a barplot)\n",
        "> 5. Based on visual inspection, does any model match human performance?\n",
        "> 6. Is either of the number conditions more difficult to process for humans or LMs?\n",
        "> 7. Select the results for Llama and humans only. Is the processing 'difficulty' of Llama correlated with the processing slowdown of humans (across singular / plural conditions)? Interpret the correlation coefficient."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SG3fmTOmlSE5"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"data/SVA_data.csv\")\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pzsErdz2lSE5"
      },
      "outputs": [],
      "source": [
        "#### YOUR CODE HERE FOR CALCULATING HYPOTHESIS METRICS AND PLOTTING ####"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pOHtPoP1lSE5"
      },
      "outputs": [],
      "source": [
        "# barplot of the results, by model and by condition (plural vs. singular)\n",
        "### YOUR CODE HERE ###\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B6Fj3KM1lSE6"
      },
      "outputs": [],
      "source": [
        "# correlation analysis\n",
        "#### YOUR CODE HERE ###"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "understanding_llms",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}