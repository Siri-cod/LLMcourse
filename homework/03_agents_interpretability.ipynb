{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Siri-cod/LLMcourse/blob/main/homework/03_agents_interpretability.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O54oDqbUnZ7I"
      },
      "source": [
        "# Homework 3: LLM agents & LLM interpretability\n",
        "\n",
        "The third homework zooms in on the following skills: implementing an advanced generation system, diving into task-specific RL fine-tuning hands-on and critically thinking about fine-tuning of LMs.\n",
        "\n",
        "### Logistics\n",
        "\n",
        "* submission deadline: June 24th th 23:59 German time via Moodle\n",
        "  * please upload a **SINGLE .IPYNB FILE named Surname_FirstName_HW3.ipynb** containing your solutions of the homework.\n",
        "* please solve and submit the homework **individually**!\n",
        "* if you use Colab, to speed up the execution of the code on Colab, you can use the available GPU (if Colab resources allow). For that, before executing your code, navigate to Runtime > Change runtime type > GPU > Save."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4jRcffT0nZ7K"
      },
      "source": [
        "## Exercise 1: Building a retrieval-augmented generation system (30 points)\n",
        "\n",
        "An increasingly popular approach to language generation is so called *retrieval-augmented generation* (RAG) wherein a language model is supplied with additional (textual) information retrieved from some storage, in addition to the actual task query. It has been found that this additional context improves model performance, and, e.g., allows to use LLMs with custom information (e.g., proprietary documents etc).\n",
        "\n",
        "The general set up of a RAG system is as follows:\n",
        "1. Some form of a database (DB) with (searchable) relevant background information (e.g., a database, a set of documents, ...) is created.\n",
        "   1. A common database format are *vector DBs*, or, vectore stores. You can optionally learn more about vector DBs, e.g., here: https://www.pinecone.io/learn/vector-database/. The important conceptual point is that some form of a searchable database with relevant (textual) information is created.\n",
        "2. An LLM that will be generating the responses to the queries, given context, is chosen.\n",
        "3. An embedding model is chosen.\n",
        "4. Task queries (e.g., questions or instructions) are provided to the system.\n",
        "   1. The query is converted to an embedding (using the model chosen ins tep 3), and the embedding is used to search and retrieve relevant information from the database. The specific retrieval method depnds on the nature of the database.\n",
        "   2. The relevant information is supplied to the LLM as context.\n",
        "5. Given the extended context, the LLM provides output.\n",
        "\n",
        "This is visualized in the figure below.\n",
        "\n",
        "![img](https://github.com/CogSciPrag/Understanding-LLMs-course/blob/main/understanding-llms/tutorials/pics/basic_rag.png?raw=1)\n",
        "\n",
        "The image is sourced from [here](https://docs.llamaindex.ai/en/stable/getting_started/concepts/).\n",
        "\n",
        "For more details on RAG, you can read the first part of [this](https://docs.llamaindex.ai/en/stable/getting_started/concepts/) blog post (until \"important concepts within each step\"). [Here](https://arxiv.org/pdf/2005.11401) is an optional paper about RAG, in case you want to learn more.\n",
        "\n",
        "**YOUR TASK**\n",
        "> Your task in this exercise is to explore RAG by implementing a RAG system for recipe generation. The implemented RAG system should be compared to the performance of the same model in a \"vanilla\" set-up where the model solves the task directly.\n",
        ">\n",
        "> We will use the package `LlamaIndex` and the LLM `microsoft/Phi-4-mini-instruct` model as the backbone for the implementation. We will use the `BAAI/bge-small-en-v1.5` model as our embedding model.\n",
        ">\n",
        "> We will use unstructured data in the form of a recipe dataset `m3hrdadfi/recipe_nlg_lite`. This dataset will be indexed and it will be used to supplement information for the LLM, additionally to the query. The train split of the dataset should be used for the index, and a sample from the test dataset will be used for sampling queries with which the system will be tested.\n",
        ">\n",
        "> For this task, please complete the following steps:\n",
        "> 1. Download the dataset from Huggingface.\n",
        "> 2. Briefly familiarize yourself with the dataset.\n",
        "> 3. Briefly familiarize yourself with [this](https://docs.llamaindex.ai/en/stable/getting_started/starter_example_local/) LLamaIndex example RAG system.  \n",
        "> 4. Complete the code below (in place of \"### YOUR CODE HERE ####\"), following the instructions in the comments to build a working RAG system that will generate recipes. Note that you will have to work with the LlamaIndex documentation to complete and understand the code. Some links are already provided.\n",
        "> 5. Answer the questions at the end of the exercise."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_GqbzoVKnZ7K"
      },
      "outputs": [],
      "source": [
        "# uncomment and run in your environment / on Colab, if you haven't installed these packages yet\n",
        "!pip install llama-index-embeddings-huggingface\n",
        "!pip install llama-index-llms-huggingface\n",
        "!pip install sentence-transformers\n",
        "!pip install datasets\n",
        "!pip install llama-index\n",
        "!pip install \"huggingface_hub[inference]\"\n",
        "!pip install accelerate bitsandbytes\n",
        "!pip install --upgrade datasets\n",
        "from IPython.display import clear_output\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YBCisI04nZ7L"
      },
      "outputs": [],
      "source": [
        "# import packages\n",
        "from datasets import load_dataset\n",
        "import os\n",
        "import pandas as pd\n",
        "from llama_index.core import VectorStoreIndex, Settings, Document\n",
        "# from llama_index.embeddings.ollama import OllamaEmbedding\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "from llama_index.llms.huggingface import HuggingFaceLLM\n",
        "from transformers import AutoTokenizer\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZBvWRLpLnZ7L"
      },
      "outputs": [],
      "source": [
        "# load dataset from HF\n",
        "dataset = load_dataset(\"m3hrdadfi/recipe_nlg_lite\")\n",
        "# convert train split to pandas dataframe\n",
        "dataset_df = pd.DataFrame(dataset[\"train\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4bTFDmjznZ7L"
      },
      "outputs": [],
      "source": [
        "# explore\n",
        "dataset_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SZXI4IxtnZ7M"
      },
      "outputs": [],
      "source": [
        "\n",
        "# 1. In order to construct a VectorStorageIndex with the texts from the train dataset split, we need to\n",
        "# create list of formatted texts.\n",
        "# We want to construct texts of the form: \"Name of recipe \\n\\n ingredients \\n\\n steps\"\n",
        "\n",
        "\n",
        "texts = [\n",
        "    #### YOUR CODE HERE #####\n",
        "]\n",
        "texts[:2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k5g-yBV5nZ7M"
      },
      "outputs": [],
      "source": [
        "# 2. We construct single Documents from the texts\n",
        "# these documents will be used to construct the vector database\n",
        "documents = [Document(text=t) for t in texts]\n",
        "documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H0_Q3ETQnZ7M"
      },
      "outputs": [],
      "source": [
        "# 3. We prepare some utility functions which are required for the LLM to generate maximally accurate responses\n",
        "# this includes correctly formatting the query and the context into the prompt and special tokens\n",
        "# that are expected by the chosen LLM backbone.\n",
        "\n",
        "# we format the texts into the Phi-4 prompt format\n",
        "# See https://huggingface.co/microsoft/Phi-4-mini-instruct\n",
        "# to heck here how the prompt should look like!\n",
        "def completion_to_prompt(completion):\n",
        "\n",
        "    return ### YOUR CODE HERE ###\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n6kfmuNfnZ7N"
      },
      "source": [
        "In the next cell, the RAG building blocks are put together. Your task is to find out what the different configurations mean and correctly complete the code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fxWhIjMknZ7N"
      },
      "outputs": [],
      "source": [
        "# 4. Save setting that are reused by our RAG system across queries\n",
        "# you can learn more about the Settings object here: https://docs.llamaindex.ai/en/stable/module_guides/supporting_modules/settings/\n",
        "\n",
        "# the embedding model is defined\n",
        "Settings.embed_model = HuggingFaceEmbedding(\n",
        "    ### YOUR CODE HERE ###\n",
        "    model_name=,\n",
        ")\n",
        "\n",
        "# backbone LLM is passed to the settings\n",
        "# this is actually the model that is used to generate the response to the query, given retrieved info\n",
        "# https://docs.llamaindex.ai/en/stable/understanding/using_llms/using_llms/\n",
        "# and here: https://docs.llamaindex.ai/en/stable/module_guides/models/llms/usage_custom/\n",
        "Settings.llm = HuggingFaceLLM(\n",
        "    ### YOUR CODE HERE ###\n",
        "    model_name= ,\n",
        "    ### YOUR CODE HERE ###\n",
        "    tokenizer_name= , #\n",
        "    #### YOUR CODE HERE ###\n",
        "    context_window=1024,\n",
        "    max_new_tokens=128,\n",
        "    generate_kwargs={\"temperature\": 0.7, \"do_sample\": True},\n",
        "    completion_to_prompt=completion_to_prompt,\n",
        "    device_map=\"auto\",\n",
        "    model_kwargs={\"torch_dtype\": torch.float16, \"load_in_8bit\": True, \"trust_remote_code\": True},\n",
        ")\n",
        "print(\"Set LLM!\")\n",
        "\n",
        "# https://docs.llamaindex.ai/en/stable/module_guides/indexing/vector_store_index/\n",
        "# we create a vector store from our documents\n",
        "# here, we let the VectorStore convert the documents to nodes automatically\n",
        "index = VectorStoreIndex.from_documents(\n",
        "    #### YOUR CODE HERE ###\n",
        ")\n",
        "print(\"Created index!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eBzEQsS9nZ7N"
      },
      "source": [
        "Below is a single example for running a query with the RAG system, and inspecting various interesting aspects of the response generated by the model. Your task is, in the following, to set up a testing loop, which will test different queries with the RAG system and vanilla generation with the same LLM. Use the example as help. Provide comments explaning the single paramters for the following example, in place of \"### YOUR COMMENT HERE ###\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SjRUvY0YnZ7N"
      },
      "outputs": [],
      "source": [
        "# https://docs.llamaindex.ai/en/stable/module_guides/deploying/query_engine/\n",
        "# we define the query engine: generic interface that allows to ask questions over data\n",
        "query_engine = index.as_query_engine(\n",
        "    ### YOUR COMMENT HERE ###\n",
        "    response_mode=\"compact\",\n",
        "    ### YOUR COMMENT HERE ###\n",
        "    similarity_top_k=3,\n",
        "    verbose=True,\n",
        ")\n",
        "# https://docs.llamaindex.ai/en/stable/module_guides/querying/response_synthesizers/\n",
        "response = query_engine.query(\"How do I make pork chop noodle soup?\")\n",
        "print(response)\n",
        "\n",
        "for i, n in enumerate(response.source_nodes):\n",
        "    print(f\"----- Node {i} -----\")\n",
        "    print(n.node.get_content())\n",
        "    print(\"score\")\n",
        "    print(n.score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HB3c9xu8nZ7N"
      },
      "outputs": [],
      "source": [
        "# testing loop\n",
        "rag_responses = []\n",
        "vanilla_responses = []\n",
        "retrieved_node_texts = []\n",
        "retrieved_node_scores = []\n",
        "\n",
        "# retrieve 20 random dish names from test dataset to test the system on\n",
        "test_df = pd.DataFrame(dataset[\"test\"]).sample(20)\n",
        "test_queries = [\n",
        "    f'How do I make {r[\"name\"]}?' for\n",
        "    _, r in test_df.iterrows()\n",
        "]\n",
        "print(test_queries[:5])\n",
        "\n",
        "for query in test_queries[:5]:\n",
        "    ### YOUR CODE HERE ###\n",
        "    # run the query against the RAG system\n",
        "    response_rag =\n",
        "    rag_responses.append(str(response_rag))\n",
        "\n",
        "    # record the texts of the nodes that were retrieved for this query\n",
        "    retrieved_node_texts.append(\n",
        "        [### YOUR CODE HERE ### ]\n",
        "    )\n",
        "\n",
        "    # record the scores of the texts of the retrieved nodes\n",
        "    retrieved_node_scores.append(\n",
        "        [### YOUR CODE HERE ###]\n",
        "    )\n",
        "    ### YOUR CODE HERE ###\n",
        "    # implement the \"vanilla\" (i.e., straightforward) generation of the response to the same query with the backbone LLM\n",
        "    # Hint: check the intro-to-hf sheet for examples how to generate text with an LM\n",
        "    response_vanilla =\n",
        "    vanilla_responses.append(response_vanilla)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qc2HrCegnZ7O"
      },
      "outputs": [],
      "source": [
        "retrieved_node_scores\n",
        "test_queries[:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WcMwq9KLnZ7O"
      },
      "source": [
        "> **Questions:**\n",
        ">\n",
        "> 1. Inspect the results of the testing. (a) How often do you prefer the RAG response over the vanilla response? (b) Do you observe differences between the RAG and vanilla responses? If yes, what are these? (c) Inpsect the retrieved documents and their scores. Do they make sense for the queries? Do the scores match your intuition about their relevance for the query?\n",
        "> 2. What could be advantages and disadvantages of using RAG? Name 1 each.\n",
        "> 3. What is the difference between documents and nodes in the RAG system?\n",
        "> 5. What does the embedding model do? What is the measure that underlies retrieval of relevant documents?\n",
        "> 6. What are different response modes of the query engine? Is the chosen mode a good choice for our application? Why (not)?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uDoMJVvtjA5D"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l5WVpulwnZ7O"
      },
      "source": [
        "## Exercise 2: Probing LLMs' grammatical knowledge (15 points)\n",
        "\n",
        "In this task, you will conduct a probing experiment to investigate whether `EleutherAI/pythia-160m` has learned the notion of subject-verb agreement. That is, we want to probe whether the model's representations encode if a sentence is grammatical (e.g., The keys to the kabinet are on the table)  or ungrammatical (e.g., The keys to the kabinet is on the table).\n",
        "\n",
        "To this end, we want to train a probing classifier on the hidden representations of the sentence and then test it on some test sentences. Furthermore, we want to compare whether the grammatical information is represented more reliably in the last layer of the model, compared to the third layer of the model.\n",
        "\n",
        "**YOUR TASK**\n",
        "> Your task in this exercise is to finish implementing the probing experiment, train and evaluate the classifier, and answer the questions at the end of the exercise.\n",
        "> We will use data from one split of the [BLiMP benchmark](https://aclanthology.org/2020.tacl-1.25/) which contains examples of grammatical and ungrammatical sentences.\n",
        ">\n",
        "> For this task, please complete the following steps:\n",
        "> 1. Download the dataset from Huggingface.\n",
        "> 2. Briefly familiarize yourself with the dataset.\n",
        "> 3. Complete the code and the comments below (in place of \"### YOUR CODE / COMMENT HERE ####\"), following the instructions in the comments.\n",
        "> 4. Answer the questions at the end of the exercise.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bq-CEcUP3OG2"
      },
      "outputs": [],
      "source": [
        "# NOTE: there might be some dependency version incompatibilities, but feel free to ignore them\n",
        "# !pip install -U datasets huggingface_hub fsspec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iuiFwqbMnZ7O"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import numpy as np\n",
        "from datasets import load_dataset, concatenate_datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1vpDo88qnZ7O"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device('cuda')\n",
        "elif torch.backends.mps.is_available():\n",
        "    device = torch.device(\"mps\")\n",
        "else:\n",
        "    device = torch.device('cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L8HR1KMinZ7O"
      },
      "outputs": [],
      "source": [
        "def get_model_and_tokenizer(model_name, device, random_weights=False):\n",
        "\n",
        "    model = AutoModelForCausalLM.from_pretrained(model_name, output_hidden_states=True).to(device)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    emb_dim = model.config.hidden_size\n",
        "\n",
        "    if random_weights:\n",
        "        print('Randomizing weights')\n",
        "        model.init_weights()\n",
        "\n",
        "    return model, tokenizer, emb_dim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EhmDnQ8enZ7O"
      },
      "outputs": [],
      "source": [
        "def melt_good(example, idx):\n",
        "    return {\"id\": example[\"pair_id\"], \"label\": \"1\", \"sentence\": example[\"sentence_good\"], \"idx\": idx}\n",
        "\n",
        "def melt_bad(example, idx):\n",
        "    return ### YOUR CODE HERE ###\n",
        "\n",
        "\n",
        "def get_data():\n",
        "    # Load the dataset\n",
        "    dataset = load_dataset(\"nyu-mll/blimp\", \"regular_plural_subject_verb_agreement_1\")\n",
        "    train_data = dataset['train']\n",
        "    # transform the data to a long format where the \"bad\" and \"good\" are labels,\n",
        "    # and sentences are all in one sentence column\n",
        "    good = train_data.map(melt_good, with_indices=True)\n",
        "    bad = train_data.map(melt_bad, with_indices=True)\n",
        "    train_data = concatenate_datasets([good, bad])\n",
        "    # Split train into train and test data, 0.8 for train and 0.2 for test\n",
        "    train_test_data = ### YOUR CODE HERE ###\n",
        "    print(train_test_data)\n",
        "    return train_test_data['train'], train_test_data['test']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AHRdNm4DnZ7P"
      },
      "outputs": [],
      "source": [
        "class Classifier(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        \"\"\"\n",
        "        Initialize a linear classifier.\n",
        "        \"\"\"\n",
        "        super(Classifier, self).__init__()\n",
        "\n",
        "        ### YOUR CODE HERE ###\n",
        "\n",
        "    def forward(self, input):\n",
        "        output = ### YOUR CODE HERE ###\n",
        "        return output\n",
        "\n",
        "\n",
        "def build_classifier(emb_dim, num_labels, device='cpu'):\n",
        "\n",
        "    classifier = Classifier(emb_dim, num_labels).to(device)\n",
        "    criterion = torch.nn.CrossEntropyLoss().to(device)\n",
        "    optimizer = torch.optim.Adam(classifier.parameters())\n",
        "\n",
        "    return classifier, criterion, optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UKjVID1InZ7P"
      },
      "outputs": [],
      "source": [
        "# load the data\n",
        "train_data, test_data = get_data()\n",
        "print(\"Training sentences:\", len(train_data))\n",
        "\n",
        "# set up the classifier\n",
        "model_name = \"EleutherAI/pythia-160m\"\n",
        "# get model and tokenizer from Transformers\n",
        "model, tokenizer, emb_dim = get_model_and_tokenizer(model_name, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o_UAcwoAnZ7P"
      },
      "outputs": [],
      "source": [
        "# COMMENT the args\n",
        "def train(\n",
        "    num_epochs,\n",
        "    train_representations,\n",
        "    train_labels,\n",
        "    classifier,\n",
        "    criterion,\n",
        "    optimizer,\n",
        "    batch_size=32\n",
        "):\n",
        "\n",
        "    num_total = len(train_representations)\n",
        "    print(\"Num total: \", num_total)\n",
        "    for i in range(num_epochs):\n",
        "        total_loss = 0.\n",
        "        num_correct = 0.\n",
        "        for batch in range(0, num_total, batch_size):\n",
        "            # get the batch of representations and labels\n",
        "            ### YOUR CODE HERE ###\n",
        "            batch_repr = torch.stack(train_representations[batch: batch+batch_size])\n",
        "            batch_labels = torch.stack(train_labels[batch: batch+batch_size])\n",
        "\n",
        "            # call the training step:\n",
        "            # passing the batch through the classifier\n",
        "            # computing the loss\n",
        "            # backpropagating the loss and updating the weights\n",
        "\n",
        "            ### YOUR CODE HERE ###\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            out = classifier(batch_repr)\n",
        "            pred = out.argmax(dim=1)\n",
        "            loss = criterion(out, batch_labels)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # accumulate the loss and number of correct predictions for tracking\n",
        "            num_correct += pred.long().eq(batch_labels.long()).cpu().sum().item()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        print('Training epoch: {}, loss: {}, accuracy: {}'.format(i, total_loss/num_total, num_correct/num_total))\n",
        "    return total_loss/num_total, num_correct/num_total"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AmsEqau9nZ7P"
      },
      "outputs": [],
      "source": [
        "def evaluate(\n",
        "    test_representations,\n",
        "    test_labels,\n",
        "    classifier,\n",
        "    criterion,\n",
        "    batch_size=32\n",
        "):\n",
        "\n",
        "    num_correct = 0.\n",
        "    num_total = len(test_representations)\n",
        "    total_loss = 0.\n",
        "    with torch.no_grad():\n",
        "        for batch in range(0, num_total, batch_size):\n",
        "            # retrieve the batch of test representations and labels\n",
        "            ### YOUR CODE HERE ###\n",
        "\n",
        "            out = classifier(batch_repr)\n",
        "            pred = out.argmax(dim=1)\n",
        "            num_correct += pred.long().eq(batch_labels.long()).cpu().sum().item()\n",
        "            total_loss += criterion(out, batch_labels)\n",
        "\n",
        "    print('Testing loss: {}, accuracy: {}'.format(total_loss/num_total, num_correct/num_total))\n",
        "    return total_loss/num_total, num_correct/num_total"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ga9jrw8MnZ7O"
      },
      "outputs": [],
      "source": [
        "# this follows the HuggingFace API for pytorch-transformers\n",
        "def get_sentence_repr(sentence, model, tokenizer, model_name, device):\n",
        "    \"\"\"\n",
        "    Get representations for one sentence\n",
        "    \"\"\"\n",
        "\n",
        "    with torch.no_grad():\n",
        "        ids = tokenizer.encode(sentence)\n",
        "        input_ids = torch.tensor([ids]).to(device)\n",
        "        # retrieve the hidden states from forward call: list of torch.FloatTensor of shape (batch_size, sequence_length, hidden_size) (hidden_states at output of each layer plus initial embedding outputs)\n",
        "        all_hidden_states = ### YOUR CODE HERE ###\n",
        "        print(all_hidden_states)\n",
        "        # convert to format required for eval: numpy array of shape (num_layers, sequence_length, representation_dim)\n",
        "        all_hidden_states = [hidden_states[0].cpu().numpy() for hidden_states in all_hidden_states]\n",
        "        all_hidden_states = np.array(all_hidden_states)\n",
        "\n",
        "    return all_hidden_states"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gtz4hGE6nZ7P"
      },
      "outputs": [],
      "source": [
        "# top-level list: sentences, second-level lists: layers, third-level tensors of num_words x representation_dim\n",
        "train_sentence_representations = [get_sentence_repr(sentence, model, tokenizer, model_name, device)\n",
        "                                  for sentence in train_data['sentence']]\n",
        "test_sentence_representations = [get_sentence_repr(sentence, model, tokenizer, model_name, device)\n",
        "                                  for sentence in test_data['sentence']]\n",
        "\n",
        "# top-level list: layers, second-level lists: sentences\n",
        "train_sentence_representations = [list(l) for l in zip(*train_sentence_representations)]\n",
        "test_sentence_representations = [list(l) for l in zip(*test_sentence_representations)]\n",
        "\n",
        "# overage over all word represenations within a sentence (num layers, words * sentences, hidden size)\n",
        "train_representations_all = [[torch.tensor(np.mean(word_representations, 0)).to(device) for word_representations in train_layer_representations] for train_layer_representations in train_sentence_representations]\n",
        "test_representations_all = [[torch.tensor(np.mean(word_representations, 0)).to(device) for word_representations in test_layer_representations] for test_layer_representations in test_sentence_representations]\n",
        "# concatenate all labels\n",
        "train_labels = train_data['label']\n",
        "train_labels_all = [torch.tensor(int(x)).type(torch.LongTensor).to(device) for x in train_labels]\n",
        "\n",
        "test_labels = test_data['label']\n",
        "test_labels_all = [torch.tensor(int(x)).type(torch.LongTensor).to(device) for x in test_labels]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qpYRO5IunZ7P"
      },
      "outputs": [],
      "source": [
        "# build classifier\n",
        "classifier, criterion, optimizer = build_classifier(emb_dim, num_labels=2, device=device)\n",
        "\n",
        "# Take final layer representations\n",
        "train_representations_final = train_representations_all[-1]\n",
        "test_representations_final = test_representations_all[-1]\n",
        "\n",
        "# Take third layer representations (first in the list are embedding results)\n",
        "train_representations_third = train_representations_all[3]\n",
        "test_representations_third = test_representations_all[3]\n",
        "\n",
        "# train the model for 100 epochs\n",
        "train_loss, train_accuracy = ### YOUR CODE HERE ###\n",
        "# test\n",
        "test_loss, test_accuracy = evaluate(test_representations_final, test_labels_all,\n",
        "         model, tokenizer, model_name, device,\n",
        "         classifier, criterion)\n",
        "print(\"Train accuracy: {}, Test accuracy: {}\".format(train_accuracy, test_accuracy))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qqSvfC3fOzEE"
      },
      "outputs": [],
      "source": [
        "# train and test on third layer representations\n",
        "classifier_third, criterion, optimizer = build_classifier(emb_dim, num_labels=2, device=device)\n",
        "\n",
        "train_loss, train_accuracy = ### YOUR CODE HERE ###\n",
        "# test\n",
        "test_loss, test_accuracy = evaluate(test_representations_third, test_labels_all,\n",
        "         model, tokenizer, model_name, device,\n",
        "         classifier_third, criterion)\n",
        "print(\"Train accuracy: {}, Test accuracy: {}\".format(train_accuracy, test_accuracy))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S-XEd2PljA5H"
      },
      "source": [
        "> Answer the following questions:\n",
        ">\n",
        "> 1. Was the information about grammatical agreement encoded more robustly in the third or the last layers' representations? How can you tell?\n",
        ">\n",
        "> 2. What potential complication for interpreting the probing results could there be if the classifier were a more powerful model (e.g., a neural net)?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NMRTQ_p2nZ7P"
      },
      "source": [
        "## Exercise 3: Surfacing of relevant predictions over layers in different domains and prompting (10 points)\n",
        "\n",
        "In this exercise, your task is to compare how 'sensible' predictions surface in the model as the input is passed through the layers. To this end, your task is to apply the logit lens with `nnsight` for `EleutherAI/pythia-410m`, and look at the top tokens that surface as the forward pass is calculated for the sequences.\n",
        "\n",
        "**YOUR TASK**\n",
        "> 1. Please implement the early decoding pipeline and visualize the results for passing the following four prompts through the model:\n",
        ">\n",
        "> a. \"The currency in the United States of America is dollar.\"\n",
        ">\n",
        "> b. \"The result of two plus two is four.\"\n",
        ">\n",
        "> c. A prompt that includes a 3-shot example before the prompt a (you should come up with the prompt yourself).\n",
        ">\n",
        "> d. A prompt that includes a 3-shot example before the prompt b (you should come up with the prompt yourself).\n",
        ">\n",
        "> Answer the folowing questions (please write 3 sentences each max.):\n",
        ">\n",
        "> 2. Are there differences in how predictions surface for the math vs. commonsense task?\n",
        ">\n",
        "> 3. Are there differences in the predictions for few-shot prompting vs. zero-shot prompting?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PMdvB7MIQ_5s"
      },
      "outputs": [],
      "source": [
        "# !pip install nnsight"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K84seQs6RJ6g"
      },
      "outputs": [],
      "source": [
        "from IPython.display import clear_output\n",
        "from nnsight import LanguageModel\n",
        "from typing import List, Callable\n",
        "import torch\n",
        "import numpy as np\n",
        "from IPython.display import clear_output\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dZJe5aVoRUd0"
      },
      "outputs": [],
      "source": [
        "# Use the pythia model\n",
        "model = LanguageModel(\"EleutherAI/pythia-410m\", device_map=\"auto\", dispatch=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qw9sddgeSzF_"
      },
      "outputs": [],
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NQGQCvkmRbra"
      },
      "outputs": [],
      "source": [
        "prompt= ### YOUR CODE HERE ###\n",
        "layers = ### YOUR CODE HERE ###\n",
        "probs_layers = []\n",
        "\n",
        "with model.trace() as tracer:\n",
        "    with tracer.invoke(prompt) as invoker:\n",
        "        for layer_idx, layer in enumerate(layers):\n",
        "            # Process layer output through the model's head and layer normalization\n",
        "            layer_output = model.embed_out(model.gpt_neox.final_layer_norm(layer.output[0]))\n",
        "\n",
        "            # Apply softmax to obtain probabilities and save the result\n",
        "            probs = torch.nn.functional.softmax(layer_output, dim=-1).save()\n",
        "            probs_layers.append(probs)\n",
        "\n",
        "probs = torch.cat([probs.value for probs in probs_layers])\n",
        "\n",
        "# Find the maximum probability and corresponding tokens for each position\n",
        "max_probs, tokens = probs.max(dim=-1)\n",
        "\n",
        "# Decode token IDs to words for each layer\n",
        "words = [[model.tokenizer.decode(t.cpu()).encode(\"unicode_escape\").decode() for t in layer_tokens]\n",
        "    for layer_tokens in tokens]\n",
        "\n",
        "# Access the 'input_ids' attribute of the invoker object to get the input words\n",
        "input_words = [model.tokenizer.decode(t) for t in invoker.inputs[0][0][\"input_ids\"][0]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lA1GYdTIRiKQ"
      },
      "outputs": [],
      "source": [
        "import plotly.express as px\n",
        "import plotly.io as pio\n",
        "\n",
        "\n",
        "pio.renderers.default = \"colab\"\n",
        "\n",
        "fig = px.imshow(\n",
        "    max_probs.detach().cpu().numpy(),\n",
        "    x=input_words,\n",
        "    y=list(range(len(words))),\n",
        "    color_continuous_scale=px.colors.diverging.RdYlBu_r,\n",
        "    color_continuous_midpoint=0.50,\n",
        "    text_auto=True,\n",
        "    labels=dict(x=\"Input Tokens\", y=\"Layers\", color=\"Probability\")\n",
        ")\n",
        "\n",
        "fig.update_layout(\n",
        "    title='Logit Lens Visualization',\n",
        "    xaxis_tickangle=0\n",
        ")\n",
        "\n",
        "fig.update_traces(text=words, texttemplate=\"%{text}\")\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4xkE3jKlT_y0"
      },
      "outputs": [],
      "source": [
        "### YOUR CODE HERE FOR LOOKING AT OTHER PROMPT RESULTS ###"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}